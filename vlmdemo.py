# -*- coding: utf-8 -*-
"""vlmdemo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18D-49dlE7D26uz8qz0IDq22KUVNR-t9F
"""

!pip install transformers accelerate bitsandbytes sentencepiece gradio

from transformers import AutoProcessor, AutoModelForVision2Seq
import torch

model_id = "llava-hf/llava-1.5-7b-hf"   # you can switch to smaller 1.3B later

processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForVision2Seq.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)



import os
from PIL import Image

def load_dataset(root="dataset"):
    data = []
    # Iterate through potential label folders
    for label_name in os.listdir(root):
        label_folder_path = os.path.join(root, label_name)
        # Ensure it's a directory (i.e., a label folder)
        if os.path.isdir(label_folder_path):
            # Iterate through contents of the label folder
            for img_name in os.listdir(label_folder_path):
                img_path = os.path.join(label_folder_path, img_name)
                # Ensure it's a file and not a hidden file/directory
                if os.path.isfile(img_path) and not img_name.startswith('.'):
                    try:
                        data.append({
                            "image": Image.open(img_path).convert("RGB"),
                            "label": label_name
                        })
                    except Exception as e:
                        print(f"Warning: Could not process {img_path}. Error: {e}")
    return data

data = load_dataset("dataset")
len(data)

from transformers import AutoProcessor, AutoModelForVision2Seq
import torch

model_id = "llava-hf/llava-1.5-7b-hf"   # you can switch to smaller 1.3B later

processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForVision2Seq.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

def predict(image, question):
    inputs = processor(
        images=image,
        text=question,
        return_tensors="pt"
    ).to("cuda")

    out = model.generate(**inputs, max_new_tokens=50)
    return processor.decode(out[0], skip_special_tokens=True)



def predict(image, question):
    inputs = processor(
        images=image,
        text=question,
        return_tensors="pt"
    ).to("cuda")

    out = model.generate(**inputs, max_new_tokens=50)
    return processor.decode(out[0], skip_special_tokens=True)

img = data[0]["image"]
question = "USER: <image>\nWhat disease does this leaf appear to have?\nASSISTANT:"
print(predict(img, question))

import gradio as gr

def demo_fn(image, question):
    return predict(image, question)

gr.Interface(
    fn=demo_fn,
    inputs=[gr.Image(type="pil"), gr.Textbox(label="Ask something")],
    outputs="text",
    title="Mini VLM Demo"
).launch()

diseases = ["healthy", "tomato_blight", "leaf_mold", "rust"]

def classify(image):
    prompt = f"USER: <image>\nIdentify the disease from these classes: {', '.join(diseases)}. Respond with only the class name.\nASSISTANT:"
    return predict(image, prompt)

print(classify(data[3]["image"]))

from IPython.display import display

display(data[0]["image"])

def severity(image):
    prompt = "USER: <image>Estimate the disease severity in percent (0-100%). Only return a number.\nASSISTANT:"
    return predict(image, prompt)

def full_report(image):
    prompt = """USER: <image>
    You are an agricultural assistant. Analyze the uploaded leaf image and provide:

    1. Disease Name
    2. Severity Percentage (0-100%)
    3. Symptoms observed
    4. Recommended actions for farmers
    Keep it short.\nASSISTANT:
    """
    return predict(image, prompt)

print(full_report(data[0]["image"]))

print(severity(data[0]["image"]))

